{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_utils import *\n",
    "import csv\n",
    "import pandas as pd\n",
    "from tqdm.notebook import trange"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collect Different Datasets Into One Place"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folder for processed data to all be put in\n",
    "COMPILED_DATA_FOLDER = \"compiled_data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Covid-19 Fake News Competition Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "FAKENEWS_FOLDER = \"covid_fake_news/data\"\n",
    "LABELED_FAKENEWS_DATASET_FILE_NAMES = [\"Constraint_Train.csv\", \"Constraint_Val.csv\"]\n",
    "UNLABELED_FAKENEWS_DATASET_FILE_NAMES = [\"Constraint_Test.csv\"]\n",
    "\n",
    "# New name to store this data under (after combining splits and specifying labeled/unlabeled)\n",
    "COMPILED_FAKENEWS_DATASET_FILE_NAME = \"fakenews.csv\"\n",
    "\n",
    "# Original Dataset paths\n",
    "ORIG_LABELED_FAKENEWS_DATASET_PATHS = [f\"{FAKENEWS_FOLDER}/{name}\" for name in LABELED_FAKENEWS_DATASET_FILE_NAMES]\n",
    "ORIG_UNLABELED_FAKENEWS_DATASET_PATHS = [f\"{FAKENEWS_FOLDER}/{name}\" for name in UNLABELED_FAKENEWS_DATASET_FILE_NAMES]\n",
    "\n",
    "# Files for storing dataset data after combining splits, with no further processing\n",
    "RAW_LABELED_FAKENEWS_DATASET_PATH = f\"{COMPILED_DATA_FOLDER}/raw_labeled_{COMPILED_FAKENEWS_DATASET_FILE_NAME}\"\n",
    "RAW_UNLABELED_FAKENEWS_DATASET_PATH = f\"{COMPILED_DATA_FOLDER}/raw_unlabeled_{COMPILED_FAKENEWS_DATASET_FILE_NAME}\"\n",
    "\n",
    "# Files for storing dataset data after preprocessing text and labels\n",
    "PREPROCESSED_LABELED_FAKENEWS_DATASET_PATH = f\"{COMPILED_DATA_FOLDER}/preprocessed_labeled_{COMPILED_FAKENEWS_DATASET_FILE_NAME}\"\n",
    "PREPROCESSED_UNLABELED_FAKENEWS_DATASET_PATH = f\"{COMPILED_DATA_FOLDER}/preprocessed_unlabeled_{COMPILED_FAKENEWS_DATASET_FILE_NAME}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Covid-19 Fake News: Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_fakenews_dataset(source_paths, dest_path):\n",
    "    # Collect and Combine Labeled Data\n",
    "    datasets = [pd.read_csv(dataset_path) for dataset_path in source_paths]\n",
    "\n",
    "    # Drop index column    \n",
    "    for dataset in datasets:\n",
    "        dataset.drop(columns=\"id\", inplace=True)\n",
    "        \n",
    "    # Concatenate\n",
    "    labeled_dataset = pd.concat(datasets)\n",
    "\n",
    "    # Rename columns to shared format\n",
    "    labeled_dataset.rename(columns={\"tweet\": \"tweet_text\", \"label\": \"tweet_label\"}, inplace=True)\n",
    "\n",
    "    # Save\n",
    "    labeled_dataset.to_csv(dest_path, index=False)    \n",
    "\n",
    "\n",
    "\n",
    "# Labeled Data\n",
    "collect_fakenews_dataset(ORIG_LABELED_FAKENEWS_DATASET_PATHS, RAW_LABELED_FAKENEWS_DATASET_PATH)\n",
    "\n",
    "# Unlabeled Data\n",
    "collect_fakenews_dataset(ORIG_UNLABELED_FAKENEWS_DATASET_PATHS, RAW_UNLABELED_FAKENEWS_DATASET_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Covid-19 Fake News: Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_dataset = pd.read_csv(RAW_LABELED_FAKENEWS_DATASET_PATH)\n",
    "preprocess(labeled_dataset)\n",
    "labeled_dataset.to_csv(PREPROCESSED_LABELED_FAKENEWS_DATASET_PATH, index=False)\n",
    "\n",
    "unlabeled_dataset = pd.read_csv(RAW_UNLABELED_FAKENEWS_DATASET_PATH)\n",
    "preprocess(unlabeled_dataset)\n",
    "unlabeled_dataset.to_csv(PREPROCESSED_UNLABELED_FAKENEWS_DATASET_PATH, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ANTiVax Dataset\n",
    "#### Requires loading tweet info from Twitter API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ANTIVAX_FOLDER = \"ANTiVax/Labeled\"\n",
    "ANTIVAX_DATASET_FILE_NAME = \"VaxMisinfoData.csv\"\n",
    "ORIG_ANTIVAX_DATASET_PATH = f\"{ANTIVAX_FOLDER}/{ANTIVAX_DATASET_FILE_NAME}\"\n",
    "\n",
    "# File for storing dataset data after retrieval, with no further processing\n",
    "RAW_ANTIVAX_DATASET_PATH = f\"{COMPILED_DATA_FOLDER}/raw_{ANTIVAX_DATASET_FILE_NAME}\"\n",
    "\n",
    "# File for storing dataset data after preprocessing text and labels (make sure labels are 0 (real) and 1 (fake))\n",
    "PREPROCESSED_ANTIVAX_DATASET_PATH = f\"{COMPILED_DATA_FOLDER}/preprocessed_{ANTIVAX_DATASET_FILE_NAME}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANTiVax: Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I have a limited number of Twitter API requests available, so make sure this cell isn't run accidentally\n",
    "# Even though it auto-skips tweets it already stores, it would still request the many tweet ids which were inaccessible and didn't end up in the dataset\n",
    "if False:\n",
    "    # Store new data indexed by id, in case we need to load from partially completed dataset processing (since I have limited quota of tweet requests)\n",
    "    new_data = {}\n",
    "    new_data_headers = [\"tweet_id\", \"tweet_text\", \"tweet_label\", \"profile_id\", \"profile_name\", \"profile_username\", \"profile_description\", \"profile_image_url\"]\n",
    "\n",
    "    # Load partially complete list\n",
    "    if os.path.exists(RAW_ANTIVAX_DATASET_PATH):\n",
    "        new_data_df = pd.read_csv(RAW_ANTIVAX_DATASET_PATH)\n",
    "        new_data = {row[\"tweet_id\"]: [row[col] for col in new_data_headers] for i, row in new_data_df.iterrows() if isinstance(row[\"tweet_text\"], str) and row[\"tweet_text\"] != \"\"}\n",
    "    \n",
    "    # Load datasets\n",
    "    twitter = TwitterRetriever()\n",
    "    orig_data = pd.read_csv(ORIG_ANTIVAX_DATASET_PATH)\n",
    "\n",
    "    def save_new_data():\n",
    "        with open(RAW_ANTIVAX_DATASET_PATH, \"w\", newline=\"\", encoding=\"utf-8\") as fp:\n",
    "            writer = csv.writer(fp)\n",
    "            writer.writerow(new_data_headers)\n",
    "            writer.writerows(new_data.values())\n",
    "\n",
    "    pbar = trange(len(orig_data.index))\n",
    "    for i in pbar:\n",
    "        row = orig_data.iloc[i]\n",
    "        tweet_id = row[\"id\"]\n",
    "        tweet_label = row[\"is_misinfo\"]\n",
    "        \n",
    "        if tweet_id in new_data:\n",
    "            continue\n",
    "        \n",
    "        tweet_info = twitter.get_tweet_info(tweet_id)\n",
    "        if tweet_info is None: # Ignorable error\n",
    "            continue\n",
    "        \n",
    "        new_data[tweet_id] = [tweet_id, tweet_info[\"tweet_text\"], tweet_label, tweet_info[\"profile_id\"], tweet_info[\"profile_name\"], tweet_info[\"profile_username\"], tweet_info[\"profile_description\"], tweet_info[\"profile_description\"]]\n",
    "        \n",
    "        pbar.set_postfix({\"tweets\": len(new_data)})\n",
    "        if i % 1000 == 0:\n",
    "            save_new_data()\n",
    "\n",
    "    # Save data as csv\n",
    "    save_new_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANTiVax: Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(RAW_ANTIVAX_DATASET_PATH)\n",
    "preprocess(dataset)\n",
    "dataset.to_csv(PREPROCESSED_ANTIVAX_DATASET_PATH, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine All Datasets (Labeled and Unlabeled Separately)\n",
    "#### Only Tweet text and labels are shared among all datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "PREPROCESSED_LABELED_DATASET_PATHS = [\n",
    "    PREPROCESSED_LABELED_FAKENEWS_DATASET_PATH,\n",
    "    PREPROCESSED_ANTIVAX_DATASET_PATH\n",
    "]\n",
    "\n",
    "PREPROCESSED_UNLABELED_DATASET_PATHS = [\n",
    "    PREPROCESSED_UNLABELED_FAKENEWS_DATASET_PATH\n",
    "]\n",
    "\n",
    "# Output dataset file paths\n",
    "COMBINED_LABELED_DATASET_FILE_NAME = \"combined_labeled_data.csv\"\n",
    "COMBINED_UNLABELED_DATASET_FILE_NAME = \"combined_unlabeled_data.csv\"\n",
    "\n",
    "COMBINED_LABELED_DATASET_PATH = f\"{COMPILED_DATA_FOLDER}/{COMBINED_LABELED_DATASET_FILE_NAME}\"\n",
    "COMBINED_UNLABELED_DATASET_PATH = f\"{COMPILED_DATA_FOLDER}/{COMBINED_UNLABELED_DATASET_FILE_NAME}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "FORCE_RECOMPUTE = False\n",
    "\n",
    "if not os.path.exists(COMBINED_LABELED_DATASET_PATH) or FORCE_RECOMPUTE:\n",
    "    # Combine all preprocessed datasets with labels\n",
    "    labeled_datasets = [pd.read_csv(dataset) for dataset in PREPROCESSED_LABELED_DATASET_PATHS]\n",
    "\n",
    "    # Filter down to only shared columns\n",
    "    labeled_datasets = [dataset.filter([\"tweet_text\", \"tweet_label\"]) for dataset in labeled_datasets]\n",
    "\n",
    "    # Combine and save\n",
    "    combined_labeled_dataset = pd.concat(labeled_datasets)\n",
    "    combined_labeled_dataset.to_csv(COMBINED_LABELED_DATASET_PATH, index=False)\n",
    "else:\n",
    "    combined_labeled_dataset = pd.read_csv(COMBINED_LABELED_DATASET_PATH)\n",
    "\n",
    "if not os.path.exists(COMBINED_UNLABELED_DATASET_PATH) or FORCE_RECOMPUTE:\n",
    "    # Combine all preprocessed datasets, including those without labels\n",
    "    unlabeled_datasets = [pd.read_csv(dataset) for dataset in PREPROCESSED_UNLABELED_DATASET_PATHS]\n",
    "\n",
    "    # Drop label column from all labeled data\n",
    "    combined_unlabeled_dataset = combined_labeled_dataset.filter([\"tweet_text\"])\n",
    "    combined_unlabeled_dataset = pd.concat([combined_unlabeled_dataset] + unlabeled_datasets)\n",
    "    combined_unlabeled_dataset.to_csv(COMBINED_UNLABELED_DATASET_PATH, index=False)\n",
    "else:\n",
    "    combined_unlabeled_dataset = pd.read_csv(COMBINED_UNLABELED_DATASET_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split Combined Labeled Dataset into Train and Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_SET_PROPORTION = 0.5\n",
    "COMBINED_LABELED_TRAIN_DATASET_FILE_NAME = \"combined_labeled_data_train.csv\"\n",
    "COMBINED_LABELED_TEST_DATASET_FILE_NAME = \"combined_labeled_data_test.csv\"\n",
    "\n",
    "COMBINED_LABELED_TRAIN_DATASET_PATH = f\"{COMPILED_DATA_FOLDER}/{COMBINED_LABELED_TRAIN_DATASET_FILE_NAME}\"\n",
    "COMBINED_LABELED_TEST_DATASET_PATH = f\"{COMPILED_DATA_FOLDER}/{COMBINED_LABELED_TEST_DATASET_FILE_NAME}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "FORCE_RECOMPUTE = False\n",
    "\n",
    "if FORCE_RECOMPUTE or not (os.path.exists(COMBINED_LABELED_TRAIN_DATASET_PATH) and os.path.exists(COMBINED_LABELED_TEST_DATASET_PATH)):\n",
    "    # Shuffle labeled dataset\n",
    "    combined_labeled_dataset = combined_labeled_dataset.sample(frac=1).reset_index(drop=True)\n",
    "    \n",
    "    train_set_end_index = int(len(combined_labeled_dataset) * TRAIN_SET_PROPORTION)\n",
    "    combined_labeled_train_dataset = combined_labeled_dataset[:train_set_end_index]\n",
    "    combined_labeled_test_dataset = combined_labeled_dataset[train_set_end_index:]\n",
    "    \n",
    "    combined_labeled_train_dataset.to_csv(COMBINED_LABELED_TRAIN_DATASET_PATH, index=False)\n",
    "    combined_labeled_test_dataset.to_csv(COMBINED_LABELED_TEST_DATASET_PATH, index=False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "75408f915446face68ce1e5da0092d91ff11efe29283027472408432cdce3ce9"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('cs329t_project')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
