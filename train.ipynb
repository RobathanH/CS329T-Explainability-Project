{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Trainer\n",
    "### Finetunes an NLP Classifier on the COVID-19 Fake News dataset, saving the best-performing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AdamW\n",
    "\n",
    "from tqdm.notebook import tqdm, trange\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_utils import *\n",
    "from model_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File Constants\n",
    "DATA_FOLDER = \"compiled_data\"\n",
    "MODEL_FOLDER = \"models\"\n",
    "\n",
    "TRAIN_DATASET_FILENAME = \"combined_labeled_data_train.csv\"\n",
    "MODEL_OUTPUT_FILENAME = \"BERT_limited.ckpt\"\n",
    "\n",
    "\n",
    "TRAIN_DATASET_PATH = f\"{DATA_FOLDER}/{TRAIN_DATASET_FILENAME}\"\n",
    "MODEL_OUTPUT_PATH = f\"{MODEL_FOLDER}/{MODEL_OUTPUT_FILENAME}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Constants\n",
    "VAL_DATA_PROPORTION = 0.4\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 15\n",
    "LR = 2e-5\n",
    "WEIGHT_DECAY = 1e-2\n",
    "NO_DECAY_PARAMS = ['bias', 'gamma', 'beta']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect training data\n",
    "data = pd.read_csv(TRAIN_DATASET_PATH)\n",
    "\n",
    "# Shuffle and split into train and val\n",
    "data = data.sample(frac=1).reset_index(drop=True)\n",
    "val_end_index = int(len(data) * VAL_DATA_PROPORTION)\n",
    "val_data = data[:val_end_index]\n",
    "train_data = data[val_end_index:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "INFO: Detected pytorch backend for <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>.\n",
      "INFO: Using backend Backend.PYTORCH.\n",
      "INFO: If this seems incorrect, you can force the correct backend by passing the `backend` parameter directly into your get_model_wrapper call.\n",
      "DEBUG: Input dtype was not passed in. Defaulting to `torch.float32`.\n"
     ]
    }
   ],
   "source": [
    "# Load pretrained model\n",
    "model = NLPHandler(model_savename=None)\n",
    "\n",
    "# Optimizer Setup\n",
    "param_optimizer = list(model.classifier.named_parameters())\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in NO_DECAY_PARAMS)],\n",
    "     'weight_decay_rate': WEIGHT_DECAY},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in NO_DECAY_PARAMS)],\n",
    "     'weight_decay_rate': 0.0}\n",
    "]\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1ac60819b5949e5a9af5d7af7f7acae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0de07f69c7c4d6684388b39bb7d1556",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/6319 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40ff14181bb54b1b894a0d8a504c2346",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/4212 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Train Loss = 0.26384, Val Accuracy = 0.95608\n",
      "Saving Model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6be2a8d65024d14b8a9458d50d86037",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/6319 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23b6c1e66fc5423b93c86b86ccaf29cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/4212 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 0.09860, Val Accuracy = 0.94682\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f34cca5edeb744de9a90c247aa778569",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/6319 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4dcd155a466455bbb2c5110de773b79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/4212 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Train Loss = 0.04793, Val Accuracy = 0.95679\n",
      "Saving Model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe8411904b93479a91bdcf8693c43fb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/6319 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce9a96c5a61149488c049b634dad4219",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/4212 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Train Loss = 0.02085, Val Accuracy = 0.95418\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "487aa7b10270438a98f0738a9a2a19c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/6319 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b7a5b6cf37b461a8541396273326918",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/4212 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Train Loss = 0.01227, Val Accuracy = 0.96534\n",
      "Saving Model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53f5c808bbbe4e5e99e0ba8a50dda559",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/6319 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c803ccfca75f4ec0b3b4cb9a09f2ee47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/4212 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train Loss = 0.00566, Val Accuracy = 0.94611\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d292e93cf9cd4457a7a0451e055afd28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/6319 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab4275b5e1a9498896f3abe568c130d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/4212 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Train Loss = 0.01297, Val Accuracy = 0.96225\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a699fb190ba4123821f4536d1e782ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/6319 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "308ed6b5351d40adae572c5f0a3ea833",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/4212 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: Train Loss = 0.00823, Val Accuracy = 0.96439\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99acc76ebe864d97a519a7856bd8bf5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/6319 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04d8cf4650ed43ba96b37c744895b01d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/4212 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: Train Loss = 0.01158, Val Accuracy = 0.95655\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73c65d5dac2b4777b291160a5e23fd69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/6319 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e497d4a41aae4e579da5d4e09ae44950",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/4212 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: Train Loss = 0.00910, Val Accuracy = 0.95964\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d12ebc53b0042859f9a1973388bdcd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/6319 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35211594db8140eb981985e67db0f538",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/4212 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: Train Loss = 0.00171, Val Accuracy = 0.96391\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81abaac142284f08afc75128b10987f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/6319 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fafd9c8a25846c0b0062b874f471b50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/4212 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: Train Loss = 0.00031, Val Accuracy = 0.96368\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "063ee1e327c04409945d5f681fbd911a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/6319 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bed24f2a602d4a88bb122769ee0dea19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/4212 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: Train Loss = 0.01083, Val Accuracy = 0.96486\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecb5e640c36d480cb80e649e3ef3b414",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/6319 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3798be2992b48d393fbdf36f1f11edd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/4212 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: Train Loss = 0.00935, Val Accuracy = 0.95845\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99d1ec01706b4ae8984967cd2791212b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/6319 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ca87c9b9955457f88a97e138d026acd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/4212 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: Train Loss = 0.00214, Val Accuracy = 0.96439\n"
     ]
    }
   ],
   "source": [
    "# Training Loop - save model with best val accuracy\n",
    "\n",
    "best_val_acc = None\n",
    "\n",
    "for i in trange(EPOCHS, desc=\"Epoch\"):\n",
    "    \n",
    "    model.classifier.train()\n",
    "    \n",
    "    total_train_loss = 0\n",
    "    \n",
    "    batch_start_index = 0\n",
    "    pbar = tqdm(initial=0, total=len(train_data), desc=\"Training\", leave=False)\n",
    "    while batch_start_index < len(train_data):\n",
    "        batch = train_data[batch_start_index : batch_start_index + BATCH_SIZE]\n",
    "        \n",
    "        loss = model.loss(batch[\"tweet_text\"].tolist(), batch[\"tweet_label\"].values)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_train_loss += loss.item() * len(batch)\n",
    "        \n",
    "        batch_start_index += len(batch)\n",
    "        pbar.update(len(batch))\n",
    "        pbar.set_postfix({\"Epoch Train Loss\": total_train_loss / batch_start_index})\n",
    "        \n",
    "    model.classifier.eval()\n",
    "    \n",
    "    total_val_acc = 0\n",
    "    \n",
    "    batch_start_index = 0\n",
    "    pbar = tqdm(initial=0, total=len(val_data), desc=\"Validating\", leave=False)\n",
    "    while batch_start_index < len(val_data):\n",
    "        batch = val_data[batch_start_index : batch_start_index + BATCH_SIZE]\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            logits = model.classify(batch[\"tweet_text\"].tolist())\n",
    "        labels = np.array(batch[\"tweet_label\"].values)\n",
    "        \n",
    "        total_val_acc += (np.argmax(logits, axis=1) == labels).sum()\n",
    "        \n",
    "        batch_start_index += len(batch)\n",
    "        pbar.update(len(batch))\n",
    "        \n",
    "    epoch_train_loss = total_train_loss / len(train_data)\n",
    "    epoch_val_acc = total_val_acc / len(val_data)\n",
    "    tqdm.write(f\"Epoch {i}: Train Loss = {epoch_train_loss:.5f}, Val Accuracy = {epoch_val_acc:.5f}\")\n",
    "    \n",
    "    if best_val_acc is None or epoch_val_acc > best_val_acc:\n",
    "        tqdm.write(\"Saving Model\")\n",
    "        torch.save(model.classifier.state_dict(), MODEL_OUTPUT_PATH)\n",
    "        best_val_acc = epoch_val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Tokenizer Setup\\nMAX_TEXT_LEN = 128\\ntokenizer = BertTokenizer.from_pretrained(\\'bert-base-uncased\\', do_lower_case=True)\\n\\ndef Encode_TextWithAttention(sentence, tokenizer, maxlen, padding_type=\\'max_length\\', attention_mask_flag=True):\\n    encoded_dict = tokenizer.encode_plus(sentence, add_special_tokens=True, max_length=maxlen, truncation=True, padding=padding_type, return_attention_mask=attention_mask_flag)\\n    return encoded_dict[\\'input_ids\\'], encoded_dict[\\'attention_mask\\']\\n\\ndef Encode_TextWithoutAttention(sentence, tokenizer, maxlen, padding_type=\\'max_length\\', attention_mask_flag=False):\\n    encoded_dict = tokenizer.encode_plus(sentence, add_special_tokens=True, max_length=maxlen, truncation=True, padding=padding_type, return_attention_mask=attention_mask_flag)\\n    return encoded_dict[\\'input_ids\\']\\n\\ndef get_TokenizedTextWithAttentionMask(sentenceList, tokenizer):\\n    token_ids_list, attention_mask_list = [], []\\n    for sentence in sentenceList:\\n        token_ids, attention_mask = Encode_TextWithAttention(sentence, tokenizer, MAX_TEXT_LEN)\\n        token_ids_list.append(token_ids)\\n        attention_mask_list.append(attention_mask)\\n    return token_ids_list, attention_mask_list\\n\\ndef get_TokenizedText(sentenceList, tokenizer):\\n    token_ids_list = []\\n    for sentence in sentenceList:\\n        token_ids = Encode_TextWithoutAttention(sentence, tokenizer, MAX_TEXT_LEN)\\n        token_ids_list.append(token_ids)\\n    return token_ids_list\\n\\n\\n\\n# Data preprocess and splitting\\nVAL_FRACTION = 0.2\\nlabeled_data = pd.read_csv(\"compiled_data/combined_labeled_data.csv\")\\nsentences, labels = labeled_data[\"tweet_text\"].values, labeled_data[\"tweet_label\"].values\\nN = len(labels)\\nrandom_order = np.random.permutation(N)\\ntrain_sentences = sentences[random_order[int(N * VAL_FRACTION):]]\\ntrain_labels = labels[random_order[int(N * VAL_FRACTION):]]\\nval_sentences = sentences[random_order[:int(N * VAL_FRACTION)]]\\nval_labels = labels[random_order[:int(N * VAL_FRACTION)]]\\n\\n\\n\\n# Create Tensor Datasets\\ntrain_token_ids, train_attention_masks = torch.tensor(get_TokenizedTextWithAttentionMask(train_sentences,tokenizer))\\nval_token_ids, val_attention_masks = torch.tensor(get_TokenizedTextWithAttentionMask(val_sentences,tokenizer))\\n\\ntrain_labels = torch.tensor(train_labels).type(torch.LongTensor)\\nval_labels = torch.tensor(val_labels).type(torch.LongTensor)\\n\\ntrain_data = TensorDataset(train_token_ids, train_attention_masks, train_labels)\\ntrain_sampler = RandomSampler(train_data)\\ntrain_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=BATCH_SIZE)\\n\\nvalidation_data = TensorDataset(val_token_ids, val_attention_masks, val_labels)\\nvalidation_sampler = SequentialSampler(validation_data)\\nvalidation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=BATCH_SIZE)\\n\\n\\n\\n# Load Model and Setup Optimizer\\n\\nmodel = BertForSequenceClassification.from_pretrained(\\'bert-base-uncased\\', num_labels=2).cuda()\\n\\nparam_optimizer = list(model.named_parameters())\\nno_decay = [\\'bias\\', \\'gamma\\', \\'beta\\']\\noptimizer_grouped_parameters = [\\n    {\\'params\\': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\\n     \\'weight_decay_rate\\': 0.01},\\n    {\\'params\\': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\\n     \\'weight_decay_rate\\': 0.0}\\n]\\noptimizer = AdamW(optimizer_grouped_parameters, lr=2e-5)\\n\\n\\n\\n# Training Loop\\n\\ndef flat_accuracy(preds, labels):\\n    pred_flat = np.argmax(preds, axis=1).flatten()\\n    labels_flat = labels.flatten()\\n    return np.sum(pred_flat == labels_flat) / len(labels_flat)\\n\\ntrain_loss_set = []\\nbest_val_accuracy = None\\n\\nfor _ in trange(EPOCHS, desc=\"Epoch\"):\\n    model.train()\\n    tr_loss_sum = 0\\n    nb_tr_examples, nb_tr_steps = 0, 0\\n  \\n    for step, batch in enumerate(tqdm(train_dataloader)):\\n        batch = tuple(t.to(device) for t in batch)\\n        b_input_ids, b_input_mask, b_labels = batch\\n        print(b_input_ids.shape, b_labels.shape)\\n        optimizer.zero_grad()\\n        outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\\n        loss = outputs[0]\\n        logits = outputs[1]\\n        train_loss_set.append(loss.item())    \\n        loss.backward()\\n        optimizer.step()\\n\\n        tr_loss_sum += loss.item()\\n        nb_tr_examples += b_input_ids.size(0)\\n        nb_tr_steps += 1\\n\\n    print(\"Train loss: {}\".format(tr_loss_sum / nb_tr_steps))\\n\\n    model.eval()\\n\\n    val_accuracy_sum = 0\\n    nb_eval_steps = 0\\n\\n    for batch in validation_dataloader:\\n        batch = tuple(t.to(device) for t in batch)\\n        b_input_ids, b_input_mask, b_labels = batch\\n        with torch.no_grad():\\n          output = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\\n          logits = output[0]\\n\\n        logits = logits.detach().cpu().numpy()\\n        label_ids = b_labels.to(\\'cpu\\').numpy()\\n\\n        val_accuracy_sum += flat_accuracy(logits, label_ids)\\n        nb_eval_steps += 1\\n\\n    val_accuracy = val_accuracy_sum / nb_eval_steps\\n    print(f\"Validation Accuracy: {val_accuracy}\")\\n    if(best_val_accuracy is None or val_accuracy >= best_val_accuracy):\\n        torch.save(model.state_dict(), f\"{MODEL_FOLDER}/BERT_extradata.ckpt\")\\n        best_val_accuracy = val_accuracy\\n        print(\\'Model Saved\\')\\n\\n\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# OLD TRAINING SCRIPT\n",
    "\n",
    "'''\n",
    "# Tokenizer Setup\n",
    "MAX_TEXT_LEN = 128\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "\n",
    "def Encode_TextWithAttention(sentence, tokenizer, maxlen, padding_type='max_length', attention_mask_flag=True):\n",
    "    encoded_dict = tokenizer.encode_plus(sentence, add_special_tokens=True, max_length=maxlen, truncation=True, padding=padding_type, return_attention_mask=attention_mask_flag)\n",
    "    return encoded_dict['input_ids'], encoded_dict['attention_mask']\n",
    "\n",
    "def Encode_TextWithoutAttention(sentence, tokenizer, maxlen, padding_type='max_length', attention_mask_flag=False):\n",
    "    encoded_dict = tokenizer.encode_plus(sentence, add_special_tokens=True, max_length=maxlen, truncation=True, padding=padding_type, return_attention_mask=attention_mask_flag)\n",
    "    return encoded_dict['input_ids']\n",
    "\n",
    "def get_TokenizedTextWithAttentionMask(sentenceList, tokenizer):\n",
    "    token_ids_list, attention_mask_list = [], []\n",
    "    for sentence in sentenceList:\n",
    "        token_ids, attention_mask = Encode_TextWithAttention(sentence, tokenizer, MAX_TEXT_LEN)\n",
    "        token_ids_list.append(token_ids)\n",
    "        attention_mask_list.append(attention_mask)\n",
    "    return token_ids_list, attention_mask_list\n",
    "\n",
    "def get_TokenizedText(sentenceList, tokenizer):\n",
    "    token_ids_list = []\n",
    "    for sentence in sentenceList:\n",
    "        token_ids = Encode_TextWithoutAttention(sentence, tokenizer, MAX_TEXT_LEN)\n",
    "        token_ids_list.append(token_ids)\n",
    "    return token_ids_list\n",
    "\n",
    "\n",
    "\n",
    "# Data preprocess and splitting\n",
    "VAL_FRACTION = 0.2\n",
    "labeled_data = pd.read_csv(\"compiled_data/combined_labeled_data.csv\")\n",
    "sentences, labels = labeled_data[\"tweet_text\"].values, labeled_data[\"tweet_label\"].values\n",
    "N = len(labels)\n",
    "random_order = np.random.permutation(N)\n",
    "train_sentences = sentences[random_order[int(N * VAL_FRACTION):]]\n",
    "train_labels = labels[random_order[int(N * VAL_FRACTION):]]\n",
    "val_sentences = sentences[random_order[:int(N * VAL_FRACTION)]]\n",
    "val_labels = labels[random_order[:int(N * VAL_FRACTION)]]\n",
    "\n",
    "\n",
    "\n",
    "# Create Tensor Datasets\n",
    "train_token_ids, train_attention_masks = torch.tensor(get_TokenizedTextWithAttentionMask(train_sentences,tokenizer))\n",
    "val_token_ids, val_attention_masks = torch.tensor(get_TokenizedTextWithAttentionMask(val_sentences,tokenizer))\n",
    "\n",
    "train_labels = torch.tensor(train_labels).type(torch.LongTensor)\n",
    "val_labels = torch.tensor(val_labels).type(torch.LongTensor)\n",
    "\n",
    "train_data = TensorDataset(train_token_ids, train_attention_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=BATCH_SIZE)\n",
    "\n",
    "validation_data = TensorDataset(val_token_ids, val_attention_masks, val_labels)\n",
    "validation_sampler = SequentialSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=BATCH_SIZE)\n",
    "\n",
    "\n",
    "\n",
    "# Load Model and Setup Optimizer\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2).cuda()\n",
    "\n",
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = ['bias', 'gamma', 'beta']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.01},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.0}\n",
    "]\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=2e-5)\n",
    "\n",
    "\n",
    "\n",
    "# Training Loop\n",
    "\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "\n",
    "train_loss_set = []\n",
    "best_val_accuracy = None\n",
    "\n",
    "for _ in trange(EPOCHS, desc=\"Epoch\"):\n",
    "    model.train()\n",
    "    tr_loss_sum = 0\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "  \n",
    "    for step, batch in enumerate(tqdm(train_dataloader)):\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        print(b_input_ids.shape, b_labels.shape)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
    "        loss = outputs[0]\n",
    "        logits = outputs[1]\n",
    "        train_loss_set.append(loss.item())    \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        tr_loss_sum += loss.item()\n",
    "        nb_tr_examples += b_input_ids.size(0)\n",
    "        nb_tr_steps += 1\n",
    "\n",
    "    print(\"Train loss: {}\".format(tr_loss_sum / nb_tr_steps))\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    val_accuracy_sum = 0\n",
    "    nb_eval_steps = 0\n",
    "\n",
    "    for batch in validation_dataloader:\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        with torch.no_grad():\n",
    "          output = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "          logits = output[0]\n",
    "\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "        val_accuracy_sum += flat_accuracy(logits, label_ids)\n",
    "        nb_eval_steps += 1\n",
    "\n",
    "    val_accuracy = val_accuracy_sum / nb_eval_steps\n",
    "    print(f\"Validation Accuracy: {val_accuracy}\")\n",
    "    if(best_val_accuracy is None or val_accuracy >= best_val_accuracy):\n",
    "        torch.save(model.state_dict(), f\"{MODEL_FOLDER}/BERT_extradata.ckpt\")\n",
    "        best_val_accuracy = val_accuracy\n",
    "        print('Model Saved')\n",
    "\n",
    "\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "75408f915446face68ce1e5da0092d91ff11efe29283027472408432cdce3ce9"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('cs329t_project')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
